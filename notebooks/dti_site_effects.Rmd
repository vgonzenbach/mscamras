---
title: "Examining site effects in DTIMSCAMRAS"
author: "Virgilio Gonzenbach"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: united
    toc: true
    toc_float: true
    toc_depth: 3
    code_folding: hide
    df_print: paged
    fig_width: 10
    fig_height: 7.5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, cache=TRUE, cache.path = here::here('notebooks/cache/dti_site_effects/'), rows.print = 15)
```

```{r, warning=FALSE}
library(tidyverse)
library(lme4)

options(scipen=999)
df = read.csv(here::here('avg_tensor_by_roi_wide.csv'),
              colClasses = c('subject' = 'factor',
                             'site' = 'factor',
                             'visit' = 'factor'))
outcomes = df %>% 
  select(where(is.numeric)) %>% 
  colnames

harmonized_df <- read.csv(here::here('results/avg_tensor_by_roi_wide_harmonized.csv'),
              colClasses = c('subject' = 'factor',
                             'site' = 'factor',
                             'visit' = 'factor'))

harmonized_no_hopkins_df <- read.csv(here::here('results/avg_tensor_by_roi_wide_no_Hopkins_harmonized.csv'),
              colClasses = c('subject' = 'factor',
                             'site' = 'factor',
                             'visit' = 'factor'))
```

## Summary 

The purpose of the DTIMSCAMRAS study is to test for site effects on DTI metrics of different brain regions in the context of a traveling subjects design with a harmonized imaging protocol. Participants (n=11) with stable Multiple Sclerosis (MS) were scanned in 4 different locations: National Institute of Health (NIH), Brigham and Women's Hospital (BWH), Johns Hopkins University (Hopkins), and The University of Pennsylvania (Penn). Two scans were collected per site visit, and participants were re-positioned between scans. Penn, BWH and NIH used Siemens scanners, while Hopkins used a Phillips scanner. The imaging modalities that were collected include T1s (MPRAGE), FLAIR, and Diffusion-weighted images (DWI). 

### Preprocessing

To preprocess imaging data, [`qsiprep`](https://qsiprep.readthedocs.io/en/latest/) was used for bias correction, skull-stripping and co-registration of the T1s and DWIs, and specialized denoising and artifact correction for the DWIs. Then, DTIFIT was used to compute the following [scalar maps](https://www.diffusion-imaging.com/2013/01/relation-between-neural-microstructure.html):  

* Fractional Anisotropy
* Mean Diffusivity
* Axial Diffusivity
* Radial Diffusivity

Concurrently, several segmentation pipelines were used on the T1s (+ FLAIRs in the case of MIMOSA) with the purpose of defining particular regions of interest (ROIs):

* White Matter and Gray Matter Segmentation methods:
  + ANTs ATROPOS
  + FSL FAST
  + Multi-Atlas Segmentation with Joint Label Fusion using the MUSE templates (JLF WM and JLF GM)
* Thalamus Segmentation
  + FSL FIRST
  + Multi-Atlas Segmentation with Joint Label Fusion using the OASIS templates (JLF THALAMUS)
* Lesion Segmentation methods:
  + MIMOSA
  
To remove lesions appearing in GM or WM (or thalamus) from the calculation, mimosa masks were inverted and applied to the segmentation label maps in order to zero out the tissue masks at the location of each lesion. These adjusted segmentation results were then used to average the DTI scalar maps across all voxels belonging to each of the ROIs. The 36 resulting DTI metrics make up the set of outcome variables which are the focus of the site effects test. Below, the outcome variables are listed. 

```{r}
data.frame(OUTCOME = outcomes) %>% 
  separate(OUTCOME, c("SEGMENTATION", "ROI", "SCALAR"), remove = FALSE)
```

Note the general <SEGMENTATION>_<ROI>_<SCALAR> format: for example, the ATROPOS_GM_AD variable denotes the average Axial Diffusivity (AD) across the Gray Matter (GM) regions defined by the Atropos Segmentation.

### Testing site effects

Testing for site effects was performed using two approaches: a permutation-based approach and a asymptotic Likelihood Ratio Test between mixed models.  

Results are presented for 4 kind of datasets:   
 
- All sites (raw): A dataset consisting observations from all the four study sites. 
- No Hopkins (raw): A dataset excluding data from Hopkins (as this data was collected using a Philips scanner and with Distortion Correction, unlikely other sites).

Longitudinal Combat (Beer et al. 2020) was performed on these two datasets leading to two more datasets to examine: 

- All sites (harmonized): A harmonized dataset including all observations
- No Hopkins (harmonized): A harmonized dataset where observations from Hopkins were excluded prior to harmonization. 

Performing the test for site effects on all these datasets allows for evaluating the effectiveness of harmonization at eliminating site effects, as well as examining to what extent Hopkins contributes to site effects in the raw and harmonized data. 

#### Permutation-Based F-test

A permutation-based F-test was used to test for site effects. For each subject, the squared difference between all possible pairs of intra-site and inter-site measures were computed. For instance, take the first row of the subject data.frame below, which has BWH as the "reference site": the squared differences between that row and the 6 non-BWH rows are computed for $y$——this process is repeated for all rows (and the results averaged) to arrive to the mean inter-site squared difference for this subject; to calculate the mean intra-site squared differences, the squared differences of the 4 intrasite pairs are averaged.

```{r}
df %>% 
  filter(subject == '01001') %>% 
  select(!where(is.numeric), ATROPOS_GM_AD) %>% 
  rename(y = ATROPOS_GM_AD)
```

These differences were averaged across all subjects resulting in the Mean Squared difference Between Sites ($\overline{MS_{B}}$) and the Mean Squared difference Within Sites ($\overline{MS_{W}}$). The test statistic is composed of their ratio:

$$
F = \frac{\overline{MS_{B}}}{\overline{MS_{W}}}
$$

For each metric, the observed test statistic was compared to a distribution of test statistics derived from 10000 permutations (i.e. a null distribution). The site labels were permuted **within each subject**. The proportion of null results higher than the observed test statistic served as a preliminary p-value $p_0$. To prevent p-values of 0 (when the observed statistic was higher than all permuted results), the p-value was shifted using the following formula:

$$ p = \frac{10000*p_0 + 1}{10000 + 1}$$

To quantify these site effects, Cohen's $f^2$ was computed from (marginal) $R^2$ values, which were calculated as presented in the work Nakagawa & Schielzeth (2013) using the function `performance::r2_nakagawa`. The (marginal) $R^2$ quantifies the proportion of variance explained by the fixed effect relative to the total variance:

$$ R^2 = \frac{\sigma_{site}^2}{\sigma_{site}^2 + \sigma_{subject}^2 + \sigma_{\epsilon}^2}$$


where:   

- $\sigma^2_{site}$ is the variance from the fixed effect component corresponding to site, see eqn. 27 in reference.
- $\sigma^2_{subject}$ is the variance component of the random effect for subject.  
- $\sigma^2_{\epsilon}$ is the variance of the residuals.

Thus, the marginal $R^2$ was computed for the following models:  
- Reduced model (denoted $a$): `outcome ~ (1 | subject)`.
- Full model (denoted $ab$): `outcome ~ (1 | subject) + (1 | site)`

And these values were used to calculated the $$f^2 = \frac{R^2_{ab} - R^2_a}{1 - R^2_{ab}} = \frac{R^2_{ab}}{1 - R^2_{ab}}$$ as the marginal $R^2 = 0$ for a model with no fixed effects (i.e. model $a$). The magnitude of the effect size was interpreted according to Cohen's (1992) guidelines: "0.02 is a small effect, 0.15 is a medium effect, and 0.35 is a large effect."

The residuals from these model $ab$ were subsequently utilized to test for the presence of multiplicative site effects (i.e. differences in site-specific variance) using the Fligner-Killeen test (Fligner, & Killeen, 1976). The FK test was chosen for its robustness under non-normality and high power for small samples (Conover, Johnson, & Johnson, 1981). 

<details>
  <summary> See details of the FK test implementation</summary>
  
  The residuals are first centered by their site-specific median and their absolute value is taken. Then, these median-centered absolute-valued residuals $x_i$ are given a score $a_i$ based on their rank (in the full sample) via the function $$ a_i = \phi^{-1} ( \frac{1}{2} + \frac{rank(x_i)}{2(N + 1)}))$$, where $\phi$ represents the normal CDF (thus $\phi^{-1}$ is its quantile function). The FK test follows a $\chi^2$ distribution with $m - 1$ degrees of freedom (for $m$ sites) and is given by the formula $$\sum_{j=1}^{m}\frac{n_j(\bar{a}_j - \bar{a} )^2}{s^2_j}$$ where   
  
- $s^2_j = \frac{1}{N - 1} (\bar{a}_j - \bar{a})^2$ is the sample variance for site $j$. 
- $\bar{a}_j$ is the mean of site $j$. 
- $\bar{a}$ is the grand mean. 
- $n_j$ is the sample size for site $j$.
  
</details>

#### Crossed-design Mixed Linear Models

As an additional test of site effects, mixed models were performed corresponding to a crossed design using the `lmer` function of the `lme4` package. For each outcome variable, two mixed-effects models were fitted: The formula for the base model includes a random intercept for subject. The formula for the extended model also includes a random intercept for site. 


Reduced Model:

```
outcome ~ (1|subject) 
```

Full model:

```
outcome ~ (1|subject) + (1|site)
```

For each of the outcome variables, these two models were compared through an asymptotic Likelihood Ratio Test  as described by Self & Liang (1987). Thus, the final p-values for these tests were calculated using a 50:50 mixture of a $\chi^2_1$ and $\chi^2_2$ distributions. 

To illustrate effect size, ICC for both site and subject random intercepts was computed from the full model using the `performance::icc` function. 95% Confidence Intervals for the ICCs were computed via bootstrapping ($B=5000$). The subject IDs were sampled with replacement. For each iteration, a bootstrap dataset was constructed by selecting all rows from the original dataset that correspond to the given subject ID. As repetition of subject IDs was possible, duplicate rows were also possible. The 2.5 and 97.5 pecentiles of the overall bootstrap distribution served as the limits of the 95% CI. 

[Source code](https://github.com/vgonzenbach/mscamras/tree/dti) for this project can be found on GitHub.

## Key Findings 

### Permutation tests

- Permutation tests revealed significant site effects in 34 out of 36 outcomes after FDR adjustment when all data is included with variable effect size (1/3 of which were larger than a medium effect). 
- After excluding Hopkins data, the number of significant site effects decreased to 25 out of 36. The site effects were almost all (34 out of 36) smaller than a medium effect. 
- Harmonization reduced to negligible quantities regardless of whether excluding Hopkins or not. However, permutation test still flagged 13 effects as significant when not excluding Hopkins. No site effects were significant when excluding Hopkins prior to harmonization. 
- After FDR correction, no multiplicative site effects were significants. 

### Mixed models

- Before harmonization there were 31 and 15 significant site effects in the "all site" data and "no hopkins" data, respectively. 
- Models run on harmonized data had trouble fitting. However, the p-values show that there were no significant site effects in either case ("all sites" and " no hopkins")

## Data Inventory

Subjects 03001 and 03002 did not complete imaging at all 4 sites. Subject 03001 completed imaging at Penn and BWH only and 03002 completed imaging at BWH, Hopkins, and the NIH only.

Subject 02001 is missing DTI data from their NIH visit.

### Scans at site per subject

```{r}
t_df <- df %>% 
   count(subject, site, .drop = FALSE) %>% 
  tidyr::pivot_wider(names_from = site, values_from = n) %>% 
  mutate(`Row Totals` = BWH + Hopkins + NIH + Penn) 
t_df <- t_df %>% 
  summarize(across(where(is.integer), sum)) %>% 
  bind_rows(t_df, .) %>% 
  mutate(across(where(is.factor), as.character))
t_df[is.na(t_df)] <- "Column Totals"
t_df
```


### For Atropos (substracting failed segmentations)

Atropos segmentation failed for 6 images: `r knitr::combine_words(c('04001NIH01', '01003NIH01', '03002NIH02', '04003NIH01', '04001BWH02', '03001BWH01'))`. 

```{r, fig.show='hold', out.width="45%", out.height="20%", fig.cap="Left: Example of successful Atropos segmentation; Right: Example of a failed Atropos segmentation with one ROI missing"}
knitr::include_graphics(here::here(c('misc/atropos_success.png', 'misc/atropos_fail.png')))
```

The following table excludes the failed atropos segmentations. Note subject 03001 only has 3 images after excluding failed atropos segmentations. 

```{r}
t_df <- df %>% 
  unite(sub, subject, site, visit, sep = '-') %>%
  filter(!sub %in% c('04001-NIH-01', '01003-NIH-01', '03002-NIH-02', '04003-NIH-01', '04001-BWH-02', '03001-BWH-01')) %>%
  separate(sub, c('subject', 'site', 'visit')) %>%
  mutate_if(is.character, as.factor) %>% 
  count(subject, site, .drop = FALSE) %>% 
  tidyr::pivot_wider(names_from = site, values_from = n) %>% 
  mutate(`Row Totals` = BWH + Hopkins + NIH + Penn)

t_df <- t_df %>% 
  summarize(across(where(is.integer), sum)) %>% 
  bind_rows(t_df, .) %>% 
  mutate(across(where(is.factor), as.character))
t_df[is.na(t_df)] <- "Column Totals"
t_df
```

The failed atropos segmentations are included in the following visualizations but were excluded for the site effects tests. 

TODO: compare harmonization with and without hopkins

## Visualizations

### Raw Data

#### Fractional Anisotropy 

##### Densities {.tabset}

Densities are colored by site, while the black line represents the overall density aggregated across sites. Distribution of the different sites tend to cluster together except for JLF Thalamus. Additionally, note the long tails caused by outliers in the ATROPOS distributions.

```{r}
plot_densities <- function(var, data){
  data %>% 
    ggplot(aes_string(x=var, color='site')) + 
    geom_density() + 
    stat_density(aes_string(x = var), geom = "line", color = "black") +
    theme_bw() + 
    xlab(str_replace_all(var, "_", " "))
}

vars <-df %>% 
  select(ends_with('FA')) %>% 
  colnames()
dplots <- lapply(vars, plot_densities, data=df) %>% 
  setNames(vars)
```

```{r, results='asis'}
for (name in names(dplots)){
  cat("######",  str_replace_all(name, "_", " "), "\n")
  print(dplots[[name]])
  cat("\n\n")
}
```

##### Box plots

Box plots show difference in medians across sites for most variables.  
Additionally, note the severe outliers in the ATROPOS metrics which correspond to the failed segmentations. For FIRST THALAMUS, subject 01002 could be considered an outlier; their FA values are consistent across sites and scans, however, suggesting this variation is meaningful and that this subject should be retained. 

```{r, warning=FALSE}
plot_avg_tensor_values <- function(tensor, data=df){
  data %>% 
    select(!is.numeric, ends_with(tensor)) %>% 
    pivot_longer(ends_with(tensor), names_to = 'seg', values_to = 'average') %>% 
    separate(seg, into = c('segmentation', 'roi', 'tensor')) %>% 
    unite(segmentation, segmentation, roi, sep = " ") %>% 
    mutate(segmentation = factor(segmentation,
                                 levels = c("ATROPOS GM", "ATROPOS WM", "FAST GM", "FAST WM",
                                            "JLF GM", "JLF WM", "JLF THALAMUS",  "FIRST THALAMUS", "MIMOSA LESION"))) %>%
    ggplot(aes(x=site, y=average)) + 
    geom_boxplot(outlier.shape = NA) + 
    geom_jitter(aes(color = subject, shape=visit), alpha=0.5) +
    #facet_grid(site~.) + 
    facet_grid(segmentation ~ .) +
    coord_flip() +
    ggtitle(sprintf("Mean %s in ROI by site", tensor)) +
    theme_bw() + 
    # theme(axis.text.x = element_text(angle = 60, vjust = 1, hjust=1)) +
    xlab("") + 
    ylab("") + 
    theme(strip.text.y = element_text(angle = 0)) + 
    scale_x_discrete(expand = c(0.15, 0.15))
}

plot_avg_tensor_values('FA')
```

#### Mean Diffusivity

##### Densities {.tabset}

For Mean Diffusivity, distributions vary more widely across sites. In particular, MIMOSA LESION MD values are quite different for Hopkins data. 


```{r}
vars <-df %>% 
  select(ends_with('MD')) %>% 
  colnames()
dplots <- lapply(vars, plot_densities, data=df) %>% 
  setNames(vars)
```

```{r, results='asis'}
for (name in names(dplots)){
  cat("######", str_replace_all(name, "_", " "), "\n")
  print(dplots[[name]])
  cat("\n\n")
}
```

##### Boxplot

As before, medians appear to be different across sites. 

```{r}
plot_avg_tensor_values('MD')
```

#### Radial Diffusivity

##### Densities {.tabset}

Radial Diffusivity also shows some variations across sites (perhaps less than MD). JLF GM RD in NIH shows multiple peaks, and JLF WM RD shows skewed distributions for all sites. As before the metric for MIMOSA shows a different distribution for Hopkins data. 

```{r}
vars <-df %>% 
  select(ends_with('RD')) %>% 
  colnames()
dplots <- lapply(vars, plot_densities, data=df) %>% 
  setNames(vars)
```

```{r, results='asis'}
for (name in names(dplots)){
  cat("######",  str_replace_all(name, "_", " "), "\n")
  print(dplots[[name]])
  cat("\n\n")
}
```

##### Boxplot

As before, medians appear to be differet across sites. 

```{r}
plot_avg_tensor_values('RD')
```

#### Axial Diffusivity

##### Densities {.tabset}

For Axial Diffusivity, Hopkins distributions show marked differences across most ROIs compared to other sites. The difference is quite pronounced for MIMOSA.  

```{r}
vars <-df %>% 
  select(ends_with('AD')) %>% 
  colnames()
dplots <- lapply(vars, plot_densities, data=df) %>% 
  setNames(vars)
```

```{r, results='asis'}
for (name in names(dplots)){
  cat("######",  str_replace_all(name, "_", " "), "\n")
  print(dplots[[name]])
  cat("\n\n")
}
```

##### Boxplot

As before, medians show differences, particularly for Hopkins.

```{r}
plot_avg_tensor_values('AD')
```

## Permutation Testing of Site Effects 

```{r}
# replace ATROPOS measures with NA for select images (segmentation failed)
fill_na_atropos <- function(data){
  atropos_cols <- df %>% 
  select(contains('ATROPOS')) %>% 
  colnames()
df[(df$subject == '04001' & df$site == 'NIH' & df$visit == '01') |
   (df$subject == '01003' & df$site == 'NIH' & df$visit == '01') |
   (df$subject == '03002' & df$site == 'NIH' & df$visit == '02') |
   (df$subject == '04003' & df$site == 'NIH' & df$visit == '01') |
   (df$subject == '04001' & df$site == 'BWH' & df$visit == '02') |
   (df$subject == '03001' & df$site == 'BWH' & df$visit == '01'), atropos_cols] <- NA
  return(df)
}

df <- fill_na_atropos(df)
#harmonized_df <- fill_na_atropos(harmonized_df)
```

The permutation-based F-test was carried out as previously described and results are presented in this section for the 4 datasets of interest. 

```{r}
ratio_stats = readRDS(here::here("results/avg_tensor_by_roi_wide_F_df.rds"))
null_dist = readRDS(here::here("results/avg_tensor_by_roi_wide_null_F_df.rds")) %>% 
  mutate(outcome = map_chr(outcome, unique)) %>% 
  select(-rowname)
```

### All sites (raw data)

#### F Distributions {.tabset}

The observed F-statistic for each of the metrics is plotted below as a black dot, while the distribution of permuted test statistics is shown as a white violin plot. All metrics showed significant site effects. 

The plot below shows that the observed F statistics are quite extreme relative to the permutation-derived distribution. The table also shows the sample-wide $\bar{MS_B}$ and $\bar{MS_W}$ values used to compute the observed F. 

##### Plot

```{r, warning=FALSE}
plot_F_dist <- function(ratio_stats, null_dist){
  null_dist %>% 
  select(outcome, F) %>% 
  unnest() %>% 
  ggplot(aes(x=outcome, y=F)) + 
  geom_violin(draw_quantiles = c(0.95)) + 
  coord_flip() + 
  geom_point(aes(x=outcome, y=F), 
             data=tibble::rownames_to_column(ratio_stats)) + 
  ggtitle("Permutation F results by outcome") + 
    ylim(c(0,22))
}

plot_F_dist(ratio_stats, null_dist)
```

##### Table

```{r}
ratio_stats %>% 
  arrange(desc(F))
```


#### p-values {.tabset}

According to the permutation F test, 34 out of 36 outcomes showed significant site effects after FDR correction. 

```{r}
#' get p value of F by comparing to permutation distribution
test_ratio_stat = function(ratio.stats, null.dists){
  
  p.vals = c()
  for (col in colnames(ratio.stats)){
    ratio.stat = pull(ratio.stats, col)
    null.dist = pull(null.dists, col)
    
    percentile = ecdf(null.dist)
    p.val <- (10000* (1 - percentile(ratio.stat)) + 1)/(1+10000)
    p.vals = c(p.vals, p.val)
  }
  names(p.vals) = colnames(ratio.stats)
  return(p.vals)
}

#' make a data.frame with p values
make_perm_df <- function(ratio_stats, null_dist){
  observed_F <- ratio_stats  %>%
  select(outcome, F) %>% 
  pivot_wider(names_from = 'outcome', values_from = 'F')


  null_Fs <- null_dist %>% 
    select(outcome, F) %>% 
    unnest() %>% 
    pivot_wider(names_from = 'outcome', values_from = 'F') %>% 
    unnest() %>% 
    select(!!!colnames(observed_F)) # re-order columns to match observed
  
  p_df <- data.frame(pval = test_ratio_stat(observed_F, null_Fs))
  
  perm_df <- cbind(
    ratio_stats %>% 
      select(F),
    p_df %>% 
      mutate(pval_fdr = p.adjust(pval, method = 'fdr'))
  )
  return(perm_df)
}

perm_df1 <- perm_df <- make_perm_df(ratio_stats, null_dist)
```

##### Plot 

Cohen's f2

```{r}
plot_p_perm <- function(perm_df){
  perm_df %>% 
    tibble::rownames_to_column('outcome') %>% 
    mutate(outcome = fct_reorder(outcome, pval_fdr, .desc = TRUE)) %>% 
    ggplot(aes(x=outcome, y=pval_fdr)) + 
    geom_point(size = 2) +
    geom_hline(yintercept = 0.05, color = 'red') + 
    coord_flip() + 
    ggtitle("P-values of Permutation Test") +
    ylim(0,1)
}

plot_p_perm(perm_df)
```

##### Table

```{r}
perm_df %>% 
  arrange(desc(pval))
```


#### Effect size: f2 {.tabset}

The Cohen's $f^2$ for the effects range from large (2), medium-to-large (9), small-to-medium (22), and small (2) as defined by the cut-offs given by Cohen (1992; see Summary). This indicated considerable variability in the magnitude of the effects but that almost 1/3 of these are larger than a medium effect. 

```{r}
### Make this into a function
# Full models

#' get f2 via Nakagawa's R2 for mixed models
#' 
#' @param data my dataframe 
#' @param r2_type the type of R2 to calculate: 'c' for conditional or 'm' for marginal (see Nakagawa, S., and Schielzeth, H. (2013) for details)
get_f2 <- function(data, r2_type = 'm'){
  run_full_models <- function(data){
    models <- outcomes %>% 
     lapply(function(x) lmer(reformulate("(1|subject) + site", x), data=data)) %>% 
     setNames(outcomes)
    return(models)
  }
  
  run_null_models <- function(data){
    models <- outcomes %>% 
     lapply(function(x) lmer(reformulate("(1|subject)", x), data=data)) %>% 
     setNames(outcomes)
    return(models)
  }
  
  get_r2 <- function(mod){
    case_when(r2_type == 'c' ~ performance::r2_nakagawa(mod)$R2_conditional, 
              r2_type == 'm' ~ performance::r2_nakagawa(mod)$R2_marginal)
  } 
  
  models <- run_full_models(data)
  
  models_0 <- run_null_models(data)
  
  R2_ab <- map_dbl(models, get_r2)
  
  R2_a <- map_dbl(models_0, get_r2)
  
  
  data.frame(f2 = (R2_ab - R2_a)/(1 - R2_ab),
             R2_ab,
             R2_a)
}

f2_df <- get_f2(df) %>% 
  arrange(desc(f2))
```

##### Plot 

```{r}
plot_f2 <- function(f2_df){
  f2_df %>% 
  rownames_to_column('outcome') %>% 
  mutate(outcome = fct_reorder(outcome, f2)) %>% 
  ggplot(aes(x=outcome, y=f2)) + 
  geom_point() +
  coord_flip() + 
  ggtitle("f2 of fixed site effect per outcome") + 
  scale_y_continuous(breaks=seq(0, 1, .1))
}

plot_f2(f2_df)
```


##### Table

```{r}
f2_df
```

#### FK test {.tabset}

The FK test revealed no significant multiplicative site effects on the residuals. 

```{r}
run_FK_test <- function(data){
  run_full_models <- function(data){
    models <- outcomes %>% 
     lapply(function(x) lmer(reformulate("(1|subject) + site", x), data=data)) %>% 
     setNames(outcomes)
    return(models)
  }

  models <- run_full_models(data)
  
  FK_df <- map(models, ~ cbind(.x@frame, data.frame(resid = residuals(.x)))) %>% # make residual column for each @frame in model output
    map(~fligner.test(.x$resid, .x$site)) %>% # run FK test on all residuals
    map(~data.frame(ChiSq = .x$statistic, DF = .x$parameter, pval = .x$p.value)) %>% # extract results from FK tests
    bind_rows(.id = 'outcome')
  
  # correct pvalues
  FK_df <- FK_df %>% 
    mutate(pval_fdr = p.adjust(pval, method='fdr'))
  rownames(FK_df) <- NULL
  return(FK_df)
}
  
df %>% 
  run_FK_test() %>% 
  arrange(pval)
```


### No Hopkins (raw)

Image acquisition at Hopkins was performed with a Philips scanner and with distortion correction during on-scanner reconstruction. As the scanner manufacturer and reconstructions method differed than other sites, here we report the permutation results based on the non-Hopkins data only.

```{r}
ratio_stats = readRDS(here::here("results/avg_tensor_by_roi_wide_no_Hopkins_F_df.rds"))
null_dist = readRDS(here::here("results/avg_tensor_by_roi_wide_no_Hopkins_null_F_df.rds"))  %>% 
  mutate(outcome = map_chr(outcome, unique)) %>% 
  select(-rowname)
```

#### F Distributions {.tabset}

As the plot show, the observed Fs are still quite extreme albeit less so relative to when all data is included. 

##### Plot

```{r, warning=FALSE}
plot_F_dist(ratio_stats, null_dist)
```

##### Table

```{r}
ratio_stats %>% 
  arrange(desc(F))
```


#### p-values {.tabset}

The p-values have been shifted away from zero. In this case, there were 25 out of 36 significant site effects (significance dropped on 9 of the tests when Hopkins is excluded).

```{r}
perm_df2 <- perm_df <- make_perm_df(ratio_stats, null_dist)
```

##### Plot 

```{r}
plot_p_perm(perm_df)
```

##### Table

```{r}
perm_df %>% 
  arrange(desc(pval))
```

#### Effect size: f2 {.tabset}

Now, effects range from large (1), medium-to-large (1), small-to-medium (17), and small (17) demonstrating an overall decrease in the magnitude of the effects: Almost all effects (34 out of 36) are smaller than a medium effect.

```{r}
f2_df <- df %>% 
  filter(site!='Hopkins') %>% 
  get_f2() %>% 
  arrange(desc(f2))
```


##### Plot 

```{r}
plot_f2(f2_df)
```


##### Table

```{r}
f2_df
```

#### FK test

Again, no significant multiplicative site effects were found. 

```{r}
df %>% 
  filter(site != 'Hopkins') %>% 
  run_FK_test() %>% 
  arrange(pval)
```

### All site  (harmonized)

In this section, we examine permutation results computed from the harmonized full dataset. 

```{r}
ratio_stats = readRDS(here::here("results/avg_tensor_by_roi_wide_harmonized_F_df.rds"))
null_dist = readRDS(here::here("results/avg_tensor_by_roi_wide_harmonized_null_F_df.rds")) %>% 
  mutate(outcome = map_chr(outcome, unique)) %>% 
  select(-rowname)
```

#### F Distributions {.tabset}

After harmonization, we observed less extreme F values compared to the two previous examples. 

##### Plot

```{r, warning=FALSE}
plot_F_dist(ratio_stats, null_dist)
```

##### Table

```{r}
ratio_stats %>% 
  arrange(desc(F))
```


#### p-values {.tabset}

Most site effects were non-significant, but the permutation shows that roughly 1/3 (13 out of 36) are still significant. 

```{r}
perm_df3 <- perm_df <- make_perm_df(ratio_stats, null_dist)
```

##### Plot 

```{r}
plot_p_perm(perm_df)
```

##### Table

```{r}
perm_df %>% 
  arrange(desc(pval))
```

#### Effect size: f2 {.tabset}

However, examining the effect size reveals that all site effects are below the 0.02 "small effect" cut off, indicating these site effects are (almost?) neglibile. 

```{r}
f2_df <- harmonized_df %>% 
  get_f2() %>% 
  arrange(desc(f2))
```


##### Plot 

```{r}
plot_f2(f2_df)
```


##### Table

```{r}
f2_df
```

#### FK test

No multiplicative site effects are significant after correcting via FDR.  

```{r}
harmonized_df %>% 
  run_FK_test() %>% 
  arrange(pval)
```

### Outcomes (failed harmonization) {.tabset}

In this section, we visually examine the pre- and post- harmonization distribution for outcomes that were flagged as having significant site effects.  We can see that the distributions are more centered toward each other after harmonization relative to pre-harmonization. But owing to multimodality in some cases, or apparent non-normality in general, there were some limitations in the degree to which long ComBat lined up the distributions. 

#### Post harmonization {.tabset}

```{r, results='asis'}
failed_harm_outcomes <- perm_df %>% 
  filter(pval_fdr <= 0.05) %>% 
  rownames()

failed_dplots_post <- lapply(failed_harm_outcomes, plot_densities, data=harmonized_df) %>% 
  setNames(failed_harm_outcomes)
failed_dplots_pre <- lapply(failed_harm_outcomes, plot_densities, data=df) %>% 
  setNames(failed_harm_outcomes)

for (name in names(failed_dplots_post)){
  cat("#####",  str_replace_all(name, "_", " "), "\n")
  print(failed_dplots_post[[name]])
  cat("\n\n")
}
```

#### Pre harmonization {.tabset}

```{r, results='asis'}
failed_dplots_pre <- lapply(failed_harm_outcomes, plot_densities, data=df) %>% 
  setNames(failed_harm_outcomes)

for (name in names(failed_dplots_pre)){
  cat("#####",  str_replace_all(name, "_", " "), "\n")
  print(failed_dplots_pre[[name]])
  cat("\n\n")
}
```

### No Hopkins  (harmonized)

The last dataset we test on is the data were Hopkins observations were excluded prior to harmonization. 

```{r}
ratio_stats = readRDS(here::here("results/avg_tensor_by_roi_wide_no_Hopkins_harmonized_F_df.rds"))
null_dist = readRDS(here::here("results/avg_tensor_by_roi_wide_no_Hopkins_harmonized_null_F_df.rds")) %>% 
  mutate(outcome = map_chr(outcome, unique)) %>% 
  select(-rowname)
```

#### F Distributions {.tabset}

Relative to other datasets, this dataset has the least extreme F values of all. 

##### Plot

```{r, warning=FALSE}
plot_F_dist(ratio_stats, null_dist)
```

##### Table

```{r}
ratio_stats %>% 
  arrange(desc(F))
```


#### p-values {.tabset}

The p-values show that when excluding Hopkins, harmonization fully corrects for site effects in all outcomes!

```{r}
perm_df4 <- perm_df <- make_perm_df(ratio_stats, null_dist)
```

##### Plot 

```{r}
plot_p_perm(perm_df)
```

##### Table

```{r}
perm_df %>% 
  arrange(desc(pval))
```

#### Effect size: f2 {.tabset}

The magnitude of the effects has shrunk toward 0 by one order of magnitude relative to the full harmonized dataset. 

```{r}
f2_df <- harmonized_no_hopkins_df %>% 
  get_f2() %>% 
  arrange(desc(f2))
```


##### Plot 

```{r}
plot_f2(f2_df)
```


##### Table

```{r}
f2_df
```

#### FK test

Again, no effects (even without correcting for multiple comparisons).

```{r}
harmonized_no_hopkins_df %>% 
  run_FK_test() %>% 
  arrange(pval)
```

### Comparison between datasets

Here, we visually compare p-values and $f^2$ for all datasets.

#### Permutation test p-values

```{r}
# plot p-values for all dataset configs
list(perm_df1, perm_df2, perm_df3, perm_df4) %>% 
  map(rownames_to_column, var = 'outcome') %>% 
  bind_rows() %>% 
  add_column(hopkins = factor(rep(c('Included', 'Excluded', 'Included', 'Excluded'), each = 36), levels = c('Included', 'Excluded')), 
             harmonized = factor(rep(c('No', 'No', 'Yes', 'Yes'), each = 36), levels = c('Yes', 'No')),
             .before='outcome') %>% 
  ggplot(aes(x=outcome, y = pval_fdr, color = harmonized, shape = hopkins)) + 
  geom_point(alpha = 1) + 
  coord_flip() + 
  scale_color_manual(values = c('dodgerblue', 'red')) + 
  scale_shape_manual(values=c(0, 4)) + 
  ggtitle('Comparison of p-values between datasets') + 
  ylab('P-value (FDR corrected)')
  
```

#### Effect size: f2

In most cases, harmonizing the full dataset was more effective than removing Hopkins data except for MIMOSA lesion metrics, most JLF GM metrics (3/4), most ATROPOS GM metrics (3/4), etc.  

```{r}
rbind(
  df %>% 
    get_f2() %>% 
  rownames_to_column(var = 'outcome'),
  df %>% 
    filter(site != 'Hopkins') %>% 
    get_f2() %>% 
  rownames_to_column(var = 'outcome'),
  harmonized_df %>% 
    get_f2() %>% 
  rownames_to_column(var = 'outcome'),
  harmonized_no_hopkins_df %>% 
    get_f2() %>% 
  rownames_to_column(var = 'outcome')
)  %>% 
  add_column(hopkins = factor(rep(c('Included', 'Excluded', 'Included', 'Excluded'), each = 36), levels = c('Included', 'Excluded')), 
             harmonized = factor(rep(c('No', 'No', 'Yes', 'Yes'), each = 36), levels = c('Yes', 'No')),
             .before='outcome') %>% 
  ggplot(aes(x=outcome, y = f2, color = harmonized, shape = hopkins)) + 
  geom_point(alpha = 1) + 
  coord_flip() + 
  scale_color_manual(values = c('dodgerblue', 'red')) + 
  scale_shape_manual(values=c(0, 4)) + 
  ggtitle('Comparison of f2 between datasets') + 
  ylab('f2 of fixed effect of site') + 
  scale_y_continuous(breaks=seq(0, 1, .1))
```

## Mixed Models

Mixed models were fitted and tested against each other as described above. 

### All Sites (raw data)

Of note, one model had issues being fit due to 0 variance. Almost all model residuals followed a normal distribution very closely. 

```{r, warning=TRUE}
# Full models
run_full_models <- function(data){
  models <- outcomes %>% 
   lapply(function(x) lmer(reformulate("(1|subject) + (1|site)", x), data=data)) %>% 
   setNames(outcomes)
}

# Null models
run_null_models <- function(data){
  models_0 <- outcomes %>% 
    lapply(function(x) lmer(reformulate("(1|subject)", x), data=data)) %>% 
    setNames(outcomes)
}

#' Test variance term
#' @param mod full model
#' @param mod_0 null model
test_variance_term <- function(mod, mod_0){
  
  A = lme4:::anova.merMod(mod, mod_0)
  lrt_stat = as.numeric(-2*(logLik(mod_0) - logLik(mod)))
  
  #if(near(lrt_stat, A$Chisq[2])) return(NA)
  
  # Naive p-value given the chi_sq(df = 1) null distribution
  pval_naive = pchisq(q = lrt_stat, df = 1, lower.tail = FALSE) 
  
  # p-value given a 50:50 mixture of chisq(1) and chisq(2),
  # according to Self & Liang 1987
  pval_SL = pchisq(q = lrt_stat, df = 1, lower.tail = FALSE)/2 +
    pchisq(q = lrt_stat, df = 2, lower.tail = FALSE)/2
  
  return(data.frame(pval_naive = pval_naive,
                    pval_SL = pval_SL))
}

# Which random effect variance is 0 
get_zero_var_model_names <- function(models){
  get_var <- function(model) summary(model)$varcor$site[1,1]
  
  zero_var_models <- models %>% 
    purrr::map_dbl(get_var) %>% 
    as.data.frame() %>% 
    setNames('variance') %>% 
    rownames_to_column(var = 'outcome') %>% 
    filter(near(variance, 0, .Machine$double.eps)) %>% 
    pull(outcome)
  return(zero_var_models)
}

models <- run_full_models(df)
models_0 <- run_null_models(df)
zero_var_model_names <- get_zero_var_model_names(models)

singular_models <- names(which(map_lgl(models, isSingular)))
```

<details>
  <summary>See models with 0 variance</summary>
  
```{r}
for (name in zero_var_model_names){
  cat('###', name, '\n\n')
  print(summary(models[[name]]))
  cat("\n\n")
}
```
  
</details>

<details>
  <summary>See models with 'singular' warning</summary>
  
```{r}
for (name in singular_models){
  cat('###', name, '\n\n')
  print(summary(models[[name]]))
  cat("\n\n")
}
```
  
</details>

<details>
  <summary>See model residuals</summary>
  
```{r,}
plot_resid <- function(mod, name) car::qqPlot(residuals(mod),dist="norm", main=glue::glue("{name} Residuals"))
imap(models, plot_resid)
```
  
</details>


#### p-values {.tabset}

31 out of 36 site effects were statistically significant. 

```{r}
make_SL_df <- function(models, models_0){
  purrr::map2(models, models_0, test_variance_term) %>% 
    setNames(outcomes) %>% 
    bind_rows(.id = 'outcome') %>% 
    #filter(!(outcome %in% zero_var_model_names)) %>% 
    select(-pval_naive) %>% 
    rename(pval = pval_SL) %>% 
    mutate(pval_fdr = p.adjust(pval, method = 'fdr')) %>% 
    arrange(pval)
}

p_df <- make_SL_df(models, models_0)

```

##### Plot

```{r}
plot_p_SL <- function(p_df){
  p_df %>% 
    mutate(outcome = fct_reorder(outcome, pval_fdr, .desc = TRUE)) %>% 
    ggplot(aes(x=outcome, y=pval_fdr)) + 
    geom_point(size = 2) +
    geom_hline(yintercept = 0.05, color = 'red') + 
    coord_flip() + 
    ggtitle("P-values of Likelihood Ratio Test (Self & Liang 1987)") +
    ylim(0,1)
}

plot_p_SL(p_df)
```

##### Table

```{r}
options(scipen = 999)
p_df
```

#### Effect size: ICC {.tabset}

Confidence intervals for site ICC show that most are non-zero. 

```{r}
options(scipen=999)
#' Get ICC for a single model
get_icc <- function(df){

  outcomes %>% 
    map(~lmer(reformulate("(1|subject) + (1|site)", .x), data=df)) %>% 
    purrr::map(~ performance::icc(.x, by_group = TRUE) %>% as.data.frame) %>% 
    setNames(outcomes) %>% 
    bind_rows(.id = "outcome") %>% 
    select_if(~ !all(is.na(.))) %>% # eliminate column of NAs
    na.omit() %>% # eliminate rows with NA
    pivot_wider(names_from = 'Group', values_from = 'ICC', names_prefix = 'ICC_') %>% 
    relocate(ICC_site, .after = 'outcome') %>%
    arrange(desc(ICC_site))
}
# extract observed icc_df
icc_df <- get_icc(df)
```

##### Site

```{r}
## Make violin plots
## AND make line
plot_ICC_site <- function(icc_df, boot_df){
  icc_df %>% 
    left_join(boot_df %>% 
                select(-ICC_site, -ICC_subject), by = 'outcome') %>% 
    mutate(outcome = fct_reorder(outcome, ICC_site)) %>% 
    ggplot(aes(x=outcome, y=ICC_site)) + 
    geom_errorbar(aes(ymin=lwr_site, ymax=uppr_site)) +
    geom_point() + 
    coord_flip() + 
    ggtitle("ICC of Site Random Effect") + 
    ylim(0,1)
}

boot_df <- readRDS(here::here('results/avg_tensor_by_roi_wide_boot_icc.rds'))
plot_ICC_site(icc_df, boot_df)
```

##### Subject

```{r}
plot_ICC_subject <- function(icc_df, boot_df){
  icc_df %>% 
    left_join(boot_df %>% 
              select(-ICC_site, -ICC_subject), by = 'outcome') %>% 
    mutate(outcome = fct_reorder(outcome, ICC_subject)) %>% 
    ggplot(aes(x=outcome, y=ICC_subject)) + 
    geom_errorbar(aes(ymin=lwr_subject, ymax=uppr_subject)) +
    geom_point() + 
    coord_flip() + 
    ggtitle("ICC of Subject Random Effect") + 
    ylim(0,1)
}
plot_ICC_subject(icc_df, boot_df)
```

##### Table

```{r}
icc_df
```

### No Hopkins (raw data)

Here, we report results from the mixed models carried out only on the non-Hopkins data. One of the models had trouble fitting due to 0 variance of the random effect for site. 

```{r, warning=FALSE}
models <- df %>% 
  filter(site != 'Hopkins') %>% 
  run_full_models()

models_0 <- df %>% 
  filter(site != 'Hopkins') %>% 
  run_null_models()

zero_var_model_names <- get_zero_var_model_names(models)
singular_models <- names(which(map_lgl(models, isSingular)))
```

<details>
  <summary>See models with 0 variance</summary>
  
```{r}
for (name in zero_var_model_names){
  cat('###', name, '\n\n')
  print(summary(models[[name]]))
  cat("\n\n")
}
```
  
</details>

<details>
  <summary>See models with 'singular' warning</summary>
  
```{r}
for (name in singular_models){
  cat('###', name, '\n\n')
  print(summary(models[[name]]))
  cat("\n\n")
}
```
  
</details>

<details>
  <summary>See model residuals</summary>
  
```{r,}
plot_resid <- function(mod, name) car::qqPlot(residuals(mod),dist="norm", main=glue::glue("{name} Residuals"))
imap(models, plot_resid)
```
  
</details>

#### p-values {.tabset}

The number of significant site effects has been reduced after excluding Hopkins data, yet 15 out of 36 remain significant. 

```{r}
p_df <- make_SL_df(models, models_0)
```

##### Plot

```{r}
plot_p_SL(p_df)
```

##### Table

```{r}
options(scipen = 999)
p_df %>% 
  arrange(pval_fdr)
```


#### Effect size: ICC {.tabset}

ICC of site across models is reduced across all features and a greater number of CIs now include 0. 


```{r}
# extract icc_df
icc_df <- df %>% 
  filter(site != 'Hopkins') %>% 
  get_icc()
```

##### Site

```{r}
options(scipen=999)
boot_df <- readRDS(here::here('results/avg_tensor_by_roi_wide_no_Hopkins_boot_icc.rds'))
plot_ICC_site(icc_df, boot_df)
```

##### Subject

```{r}
plot_ICC_subject(icc_df, boot_df)
```

##### Table

```{r}
icc_df
```

### All sites (harmonized data)

Here, we report results from the mixed models carried out only on the non-Hopkins data.

Most models had trouble fitting due to 0 variance. 

```{r, warning=TRUE}
models <- harmonized_df %>% 
  run_full_models()

models_0 <- harmonized_df %>% 
  run_null_models()

zero_var_model_names <- get_zero_var_model_names(models)
singular_models <- names(which(map_lgl(models, isSingular)))
```

<details>
  <summary>See models with 0 variance</summary>
  
```{r}
for (name in zero_var_model_names){
  cat('###', name, '\n\n')
  print(summary(models[[name]]))
  cat("\n\n")
}
```
  
</details>

<details>
  <summary>See models with 'singular' warning</summary>
  
```{r}
for (name in singular_models){
  cat('###', name, '\n\n')
  print(summary(models[[name]]))
  cat("\n\n")
}
```
  
</details>

<details>
  <summary>See model residuals</summary>
  
```{r,}
plot_resid <- function(mod, name) car::qqPlot(residuals(mod),dist="norm", main=glue::glue("{name} Residuals"))
imap(models, plot_resid)
```
  
</details>

#### p-values {.tabset}

The tests for all models are non-significant!

```{r}
p_df <- make_SL_df(models, models_0)
```

##### Plot

```{r}
plot_p_SL(p_df)
```

##### Table

```{r}
options(scipen = 999)
p_df %>% 
  arrange(pval_fdr)
```


#### Effect size: ICC {.tabset}

As most models had issues fitting due to 0 variance only 2 ICCs are shown. 

```{r}
# extract icc_df
icc_df <- harmonized_df %>% 
  get_icc()
```

##### Site

```{r}
options(scipen=999)
boot_df <- readRDS(here::here('results/avg_tensor_by_roi_wide_harmonized_boot_icc.rds'))
plot_ICC_site(icc_df, boot_df)
```

##### Subject

```{r}
plot_ICC_subject(icc_df, boot_df)
```

##### Table

```{r}
icc_df
```

### No Hopkins (harmonized)

Here, we report results from the mixed models carried out only on the non-Hopkins data. Again, most models had a zero variance and gave warnings. 

```{r}
models <- harmonized_no_hopkins_df %>% 
  run_full_models()

models_0 <- harmonized_no_hopkins_df %>% 
  run_null_models()

zero_var_model_names <- get_zero_var_model_names(models)
singular_models <- names(which(map_lgl(models, isSingular)))
```

<details>
  <summary>See models with 0 variance</summary>
  
```{r}
for (name in zero_var_model_names){
  cat('###', name, '\n\n')
  print(summary(models[[name]]))
  cat("\n\n")
}
```
  
</details>

<details>
  <summary>See models with 'singular' warning</summary>
  
```{r}
for (name in singular_models){
  cat('###', name, '\n\n')
  print(summary(models[[name]]))
  cat("\n\n")
}
```
  
</details>

<details>
  <summary>See model residuals</summary>
  
```{r,}
plot_resid <- function(mod, name) car::qqPlot(residuals(mod),dist="norm", main=glue::glue("{name} Residuals"))
imap(models, plot_resid)
```
  
</details>

#### p-values {.tabset}

As above, all site effects are non-significant. 

```{r}
p_df <- make_SL_df(models, models_0)
```

##### Plot

```{r}
plot_p_SL(p_df)
```

##### Table

```{r}
options(scipen = 999)
p_df %>% 
  arrange(pval_fdr)
```


#### Effect size: ICC {.tabset}

```{r}
# extract icc_df
icc_df <- harmonized_no_hopkins_df %>% 
  get_icc()
```

##### Site

```{r}
options(scipen=999)
boot_df <- readRDS(here::here('results/avg_tensor_by_roi_wide_no_Hopkins_harmonized_boot_icc.rds'))
plot_ICC_site(icc_df, boot_df)
```

##### Subject

```{r}
plot_ICC_subject(icc_df, boot_df)
```

##### Table

```{r}
icc_df
```

After FDR-adjustment, 14 of the 36 variables show site effects according to the mixed models run on non-Hopkins data.
