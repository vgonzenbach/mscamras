---
title: "Examining site effects in DTIMSCAMRAS"
author: "Virgilio Gonzenbach"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: united
    toc: true
    toc_float: true
    code_folding: hide
    df_print: paged
    fig_width: 10
    fig_height: 7.5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, cache=TRUE, cache.path = here::here('notebooks/cache/dti_site_effects/'), rows.print = 15)
```

```{r, warning=FALSE}
library(tidyverse)
library(lme4)

options(scipen=999)
df = read.csv(here::here('avg_tensor_by_roi_wide.csv'),
              colClasses = c('subject' = 'factor',
                             'site' = 'factor',
                             'visit' = 'factor'))
outcomes = df %>% 
  select(where(is.numeric)) %>% 
  colnames
```

## Summary 

The purpose of the DTIMSCAMRAS study is to test for site effects on DTI metrics of different brain regions in the context of a traveling subjects design with a harmonized imaging protocol. Participants (n=11) with stable Multiple Sclerosis (MS) were scanned in 4 different locations: National Institute of Health (NIH), Brigham and Women's Hospital (BWH), Johns Hopkins University (Hopkins), and The University of Pennsylvania (Penn). Two scans were collected per site visit, and participants were re-positioned between scans. Penn, BWH and NIH used Siemens scanners, while Hopkins used a Phillips scanner. The imaging modalities that were collected include T1s (MPRAGE), FLAIR, and Diffusion-weighted images (DWI). 

### Preprocessing

To preprocess imaging data, [`qsiprep`](https://qsiprep.readthedocs.io/en/latest/) was used for bias correction, skull-stripping and co-registration of the T1s and DWIs, and specialized denoising and artifact correction for the DWIs. Then, DTIFIT was used to compute the following [scalar maps](https://www.diffusion-imaging.com/2013/01/relation-between-neural-microstructure.html):  

* Fractional Anisotropy
* Mean Diffusivity
* Axial Diffusivity
* Radial Diffusivity

Concurrently, several segmentation pipelines were used on the T1s (+ FLAIRs in the case of MIMOSA) with the purpose of defining particular regions of interest (ROIs):

* White Matter and Gray Matter Segmentation methods:
  + ANTs ATROPOS
  + FSL FAST
  + Multi-Atlas Segmentation with Joint Label Fusion using the MUSE templates (JLF WM and JLF GM)
* Thalamus Segmentation
  + FSL FIRST
  + Multi-Atlas Segmentation with Joint Label Fusion using the OASIS templates (JLF THALAMUS)
* Lesion Segmentation methods:
  + MIMOSA
  
To remove lesions appearing in GM or WM (or thalamus) from the calculation, mimosa masks were inverted and applied to the segmentation label maps in order to zero out the tissue masks at the location of each lesion. These adjusted segmentation results were then used to average the DTI scalar maps across all voxels belonging to each of the ROIs. The 36 resulting DTI metrics make up the set of outcome variables which are the focus of the site effects test. Below, the outcome variables are listed. 

```{r}
data.frame(OUTCOME = outcomes) %>% 
  separate(OUTCOME, c("SEGMENTATION", "ROI", "SCALAR"), remove = FALSE)
```

Note the general <SEGMENTATION>_<ROI>_<SCALAR> format: for example, the ATROPOS_GM_AD variable denotes the average Axial Diffusivity (AD) across the Gray Matter (GM) regions defined by the Atropos Segmentation.

### Testing site effects

Testing for site effects was performed using two approaches: a permutation-based approach and a deviance test of mixed models.  

Additionally, as Hopkins data was acquired using a Philips scanner, which is in contrast to all other sites, the two testing approaches were performed again while excluding Hopkins data.

#### Permutation-Based F-test

A permutation-based F-test was used to test for site effects. For each subject, the squared difference between all possible pairs of intra-site and inter-site measures were computed. For instance, take the first row of the subject data.frame below, which has BWH as the "reference site": the squared differences between that row and the 6 non-BWH rows are computed for $y$——this process is repeated for all rows (and the results averaged) to arrive to the mean inter-site squared difference for this subject; to calculate the mean intra-site squared differences, the squared differences of the 4 intrasite pairs are averaged.

```{r}
df %>% 
  filter(subject == '01001') %>% 
  select(!where(is.numeric), ATROPOS_GM_AD) %>% 
  rename(y = ATROPOS_GM_AD)
```

These differences were averaged across all subjects resulting in the Mean Squared difference Between Sites ($\overline{MS_{B}}$) and the Mean Squared difference Within Sites ($\overline{MS_{W}}$). The test statistic is composed of their ratio:

$$
F = \frac{\overline{MS_{B}}}{\overline{MS_{W}}}
$$

For each metric, the observed test statistic was compared to a distribution of test statistics derived from 10000 permutations (i.e. a null distribution). The site labels were permuted **within each subject**. The proportion of null results higher than the observed test statistic served as a preliminary p-value $p_0$. To prevent p-values of 0 (when the observed statistic was higher than all permuted results), the p-value was shifted using the following formula:

$$ p = \frac{10000*p_0 + 1}{10000 + 1}$$

#### Crossed-design Mixed Linear Models

As an additional test of site effects, mixed models were performed corresponding to a crossed design using the `lmer` function of the `lme4` package. For each outcome variable, two mixed-effects models were fitted: The formula for the base model includes a random intercept for subject. The formula for the extended model also includes a random intercept for site. 

Base Model:

```
y ~ (1|subject) 
```

Extended model:

```
y ~ (1|subject) + (1|site)
```

For each outcome, these two models were compared through a deviance test using the `anova` function in R. P-values for these tests are reported as a test of site effects. 

[Source code](https://github.com/vgonzenbach/mscamras/tree/dti) for this project can be found on GitHub.

## Key Findings 

- Permutation tests revealed significant site effects in 31 out of 36 outcomes after FDR adjustment when all data is included. After excluding Hopkins data, the number of significant site effects decreased to 13. Most of the DTI metrics (i.e. outcomes) that showed differences were derived from thalamus segmentation results. 
- For mixed models, most outcome variables (13 out of 36) show a significant test after FDR adjustment indicating the presence of site effects when all data is included. When Hopkins data is excluded, none of the outcome variables show site effects. 

## Questions for follow up  

- It is unclear to what extent potential segmentation differences across sites might contribute to site effects, i.e. to what extent do site differences stem from ROI coverage differences (due to scanner effects on the T1s), and to what extent are the differences due to scanner effects on DWI images? 

## Data Inventory

Subjects 03001 and 03002 did not complete imaging at all 4 sites. Subject 03001 completed imaging at Penn and BWH only and 03002 completed imaging at BWH, Hopkins, and the NIH only.

Subject 02001 is missing DTI data from their NIH visit.

### Scans at site per subject

```{r}
t_df <- df %>% 
   count(subject, site, .drop = FALSE) %>% 
  tidyr::pivot_wider(names_from = site, values_from = n) %>% 
  mutate(`Row Totals` = BWH + Hopkins + NIH + Penn) 
t_df <- t_df %>% 
  summarize(across(where(is.integer), sum)) %>% 
  bind_rows(t_df, .) %>% 
  mutate(across(where(is.factor), as.character))
t_df[is.na(t_df)] <- "Column Totals"
t_df
```


### For Atropos (substracting failed segmentations)

Atropos segmentation failed for 6 images: `r knitr::combine_words(c('04001NIH01', '01003NIH01', '03002NIH02', '04003NIH01', '04001BWH02', '03001BWH01'))`. 

```{r, fig.show='hold', out.width="45%", out.height="20%", fig.cap="Left: Example of successful Atropos segmentation; Right: Example of a failed Atropos segmentation with one ROI missing"}
knitr::include_graphics(here::here(c('misc/atropos_success.png', 'misc/atropos_fail.png')))
```

The following table excludes the failed atropos segmentations. Note subject 03001 only has 3 images after excluding failed atropos segmentations. 

```{r}
t_df <- df %>% 
  unite(sub, subject, site, visit, sep = '-') %>%
  filter(!sub %in% c('04001-NIH-01', '01003-NIH-01', '03002-NIH-02', '04003-NIH-01', '04001-BWH-02', '03001-BWH-01')) %>%
  separate(sub, c('subject', 'site', 'visit')) %>%
  mutate_if(is.character, as.factor) %>% 
  count(subject, site, .drop = FALSE) %>% 
  tidyr::pivot_wider(names_from = site, values_from = n) %>% 
  mutate(`Row Totals` = BWH + Hopkins + NIH + Penn)

t_df <- t_df %>% 
  summarize(across(where(is.integer), sum)) %>% 
  bind_rows(t_df, .) %>% 
  mutate(across(where(is.factor), as.character))
t_df[is.na(t_df)] <- "Column Totals"
t_df
```

The failed atropos segmentations are included in the following visualizations but were excluded for the site effects tests. 

TODO: compare harmonization with and without hopkins

## Visualizations

### Raw Data

#### Fractional Anisotropy 

##### Densities {.tabset}

Densities are colored by site, while the black line represents the overall density aggregated across sites. Distribution of the different sites tend to cluster together except for JLF Thalamus. Additionally, note the long tails caused by outliers in the ATROPOS distributions.

```{r}
plot_densities <- function(var){
  df %>% 
    ggplot(aes_string(x=var, color='site')) + 
    geom_density() + 
    stat_density(aes_string(x = var), geom = "line", color = "black") +
    theme_bw() + 
    xlab(str_replace_all(var, "_", " "))
}

vars <-df %>% 
  select(ends_with('FA')) %>% 
  colnames()
dplots <- lapply(vars, plot_densities) %>% 
  setNames(vars)
```

```{r, results='asis'}
for (name in names(dplots)){
  cat("######",  str_replace_all(name, "_", " "), "\n")
  print(dplots[[name]])
  cat("\n\n")
}
```

##### Box plots

Box plots show difference in medians across sites for most variables.  
Additionally, note the severe outliers in the ATROPOS metrics which correspond to the failed segmentations. For FIRST THALAMUS, subject 01002 could be considered an outlier; their FA values are consistent across sites and scans, however, suggesting this variation is meaningful and that this subject should be retained. 

```{r, warning=FALSE}
plot_avg_tensor_values <- function(tensor){
  df %>% 
    select(!is.numeric, ends_with(tensor)) %>% 
    pivot_longer(ends_with(tensor), names_to = 'seg', values_to = 'average') %>% 
    separate(seg, into = c('segmentation', 'roi', 'tensor')) %>% 
    unite(segmentation, segmentation, roi, sep = " ") %>% 
    mutate(segmentation = factor(segmentation,
                                 levels = c("ATROPOS GM", "ATROPOS WM", "FAST GM", "FAST WM",
                                            "JLF GM", "JLF WM", "JLF THALAMUS",  "FIRST THALAMUS", "MIMOSA LESION"))) %>%
    ggplot(aes(x=site, y=average)) + 
    geom_boxplot(outlier.shape = NA) + 
    geom_jitter(aes(color = subject, shape=visit), alpha=0.5) +
    #facet_grid(site~.) + 
    facet_grid(segmentation ~ .) +
    coord_flip() +
    ggtitle(sprintf("Mean %s in ROI by site", tensor)) +
    theme_bw() + 
    # theme(axis.text.x = element_text(angle = 60, vjust = 1, hjust=1)) +
    xlab("") + 
    ylab("") + 
    theme(strip.text.y = element_text(angle = 0)) + 
    scale_x_discrete(expand = c(0.15, 0.15))
}

plot_avg_tensor_values('FA')
```

#### Mean Diffusivity

##### Densities {.tabset}

For Mean Diffusivity, distributions vary more widely across sites. In particular, MIMOSA LESION MD values are quite different for Hopkins data. 


```{r}
vars <-df %>% 
  select(ends_with('MD')) %>% 
  colnames()
dplots <- lapply(vars, plot_densities) %>% 
  setNames(vars)
```

```{r, results='asis'}
for (name in names(dplots)){
  cat("######", str_replace_all(name, "_", " "), "\n")
  print(dplots[[name]])
  cat("\n\n")
}
```

##### Boxplot

As before, medians appear to be different across sites. 

```{r}
plot_avg_tensor_values('MD')
```

#### Radial Diffusivity

##### Densities {.tabset}

Radial Diffusivity also shows some variations across sites (perhaps less than MD). JLF GM RD in NIH shows multiple peaks, and JLF WM RD shows skewed distributions for all sites. As before the metric for MIMOSA shows a different distribution for Hopkins data. 

```{r}
vars <-df %>% 
  select(ends_with('RD')) %>% 
  colnames()
dplots <- lapply(vars, plot_densities) %>% 
  setNames(vars)
```

```{r, results='asis'}
for (name in names(dplots)){
  cat("######",  str_replace_all(name, "_", " "), "\n")
  print(dplots[[name]])
  cat("\n\n")
}
```

##### Boxplot

As before, medians appear to be differet across sites. 

```{r}
plot_avg_tensor_values('RD')
```

#### Axial Diffusivity

##### Densities {.tabset}

For Axial Diffusivity, Hopkins distributions show marked differences across most ROIs compared to other sites. The difference is quite pronounced for MIMOSA.  

```{r}
vars <-df %>% 
  select(ends_with('AD')) %>% 
  colnames()
dplots <- lapply(vars, plot_densities) %>% 
  setNames(vars)
```

```{r, results='asis'}
for (name in names(dplots)){
  cat("######",  str_replace_all(name, "_", " "), "\n")
  print(dplots[[name]])
  cat("\n\n")
}
```

##### Boxplot

As before, medians show differences, particularly for Hopkins.

```{r}
plot_avg_tensor_values('AD')
```

### Harmonized Data

```{r}
## NOT INCLUDED !!!!!!-----------


#### Combat estimates
# load harmonized data
harmonized_df <- read.csv(here::here('results/avg_tensor_by_roi_wide_harmonized.csv'),
                          colClasses = c('subject' = 'character',
                             'site' = 'character',
                             'visit' = 'character'))
harmonized_no_hopkins_df <- read.csv(here::here('results/avg_tensor_by_roi_wide_no_Hopkins_harmonized.csv'))

# load models
combat_model <- readRDS(here::here('results/avg_tensor_by_roi_wide_model.csv'))[c("gammahat", "delta2hat", "gammastarhat", "delta2starhat")]
combat_model_no_hopkins <- readRDS(here::here('results/avg_tensor_by_roi_wide_no_Hopkins_model.csv'))[c("gammahat", "delta2hat", "gammastarhat", "delta2starhat")]

# model parameters
# what's the best way to plot combat estimates
```

#### Fractional Anisotropy 

##### Densities {.tabset}

Densities are colored by site, while the black line represents the overall density aggregated across sites. Distribution of the different sites tend to cluster together except for JLF Thalamus. Additionally, note the long tails caused by outliers in the ATROPOS distributions.

```{r}
plot_densities <- function(var){
  harmonized_df %>% 
    ggplot(aes_string(x=var, color='site')) + 
    geom_density() + 
    stat_density(aes_string(x = var), geom = "line", color = "black") +
    theme_bw() + 
    xlab(str_replace_all(var, "_", " "))
}

vars <- harmonized_df %>% 
  select(ends_with('FA')) %>% 
  colnames()
dplots <- lapply(vars, plot_densities) %>% 
  setNames(vars)
```

```{r, results='asis'}
for (name in names(dplots)){
  cat("######",  str_replace_all(name, "_", " "), "\n")
  print(dplots[[name]])
  cat("\n\n")
}
```

##### Box plots

Box plots show difference in medians across sites for most variables.  
Additionally, note the severe outliers in the ATROPOS metrics which correspond to the failed segmentations. For FIRST THALAMUS, subject 01002 could be considered an outlier; their FA values are consistent across sites and scans, however, suggesting this variation is meaningful and that this subject should be retained. 

```{r, warning=FALSE}
plot_avg_tensor_values <- function(tensor){
  harmonized_df %>% 
    select(!is.numeric, ends_with(tensor)) %>% 
    pivot_longer(ends_with(tensor), names_to = 'seg', values_to = 'average') %>% 
    separate(seg, into = c('segmentation', 'roi', 'tensor')) %>% 
    unite(segmentation, segmentation, roi, sep = " ") %>% 
    mutate(segmentation = factor(segmentation,
                                 levels = c("ATROPOS GM", "ATROPOS WM", "FAST GM", "FAST WM",
                                            "JLF GM", "JLF WM", "JLF THALAMUS",  "FIRST THALAMUS", "MIMOSA LESION"))) %>%
    ggplot(aes(x=site, y=average)) + 
    geom_boxplot(outlier.shape = NA) + 
    geom_jitter(aes(color = subject, shape=visit), alpha=0.5) +
    #facet_grid(site~.) + 
    facet_grid(segmentation ~ .) +
    coord_flip() +
    ggtitle(sprintf("Mean %s in ROI by site", tensor)) +
    theme_bw() + 
    # theme(axis.text.x = element_text(angle = 60, vjust = 1, hjust=1)) +
    xlab("") + 
    ylab("") + 
    theme(strip.text.y = element_text(angle = 0)) + 
    scale_x_discrete(expand = c(0.15, 0.15))
}

plot_avg_tensor_values('FA')
```

#### Mean Diffusivity

##### Densities {.tabset}

For Mean Diffusivity, distributions vary more widely across sites. In particular, MIMOSA LESION MD values are quite different for Hopkins data. 


```{r}
vars <- harmonized_df %>% 
  select(ends_with('MD')) %>% 
  colnames()
dplots <- lapply(vars, plot_densities) %>% 
  setNames(vars)
```

```{r, results='asis'}
for (name in names(dplots)){
  cat("######", str_replace_all(name, "_", " "), "\n")
  print(dplots[[name]])
  cat("\n\n")
}
```

##### Boxplot

As before, medians appear to be different across sites. 

```{r}
plot_avg_tensor_values('MD')
```

#### Radial Diffusivity

##### Densities {.tabset}

Radial Diffusivity also shows some variations across sites (perhaps less than MD). JLF GM RD in NIH shows multiple peaks, and JLF WM RD shows skewed distributions for all sites. As before the metric for MIMOSA shows a different distribution for Hopkins data. 

```{r}
vars <-harmonized_df %>% 
  select(ends_with('RD')) %>% 
  colnames()
dplots <- lapply(vars, plot_densities) %>% 
  setNames(vars)
```

```{r, results='asis'}
for (name in names(dplots)){
  cat("######",  str_replace_all(name, "_", " "), "\n")
  print(dplots[[name]])
  cat("\n\n")
}
```

##### Boxplot

As before, medians appear to be differet across sites. 

```{r}
plot_avg_tensor_values('RD')
```

#### Axial Diffusivity

##### Densities {.tabset}

For Axial Diffusivity, Hopkins distributions show marked differences across most ROIs compared to other sites. The difference is quite pronounced for MIMOSA.  

```{r}
vars <-harmonized_df %>% 
  select(ends_with('AD')) %>% 
  colnames()
dplots <- lapply(vars, plot_densities) %>% 
  setNames(vars)
```

```{r, results='asis'}
for (name in names(dplots)){
  cat("######",  str_replace_all(name, "_", " "), "\n")
  print(dplots[[name]])
  cat("\n\n")
}
```

##### Boxplot

As before, medians show differences, particularly for Hopkins.

```{r}
plot_avg_tensor_values('AD')
```

## Permutation Testing of Site Effects 

```{r}
# replace ATROPOS measures with NA for select images (segmentation failed)
fill_na_atropos <- function(df){
  atropos_cols <- df %>% 
  select(contains('ATROPOS')) %>% 
  colnames()
df[(df$subject == '04001' & df$site == 'NIH' & df$visit == '01') |
   (df$subject == '01003' & df$site == 'NIH' & df$visit == '01') |
   (df$subject == '03002' & df$site == 'NIH' & df$visit == '02') |
   (df$subject == '04003' & df$site == 'NIH' & df$visit == '01') |
   (df$subject == '04001' & df$site == 'BWH' & df$visit == '02') |
   (df$subject == '03001' & df$site == 'BWH' & df$visit == '01'), atropos_cols] <- NA
  return(df)
}

df <- fill_na_atropos(df)
harmonized_df <- fill_na_atropos(harmonized_df)
```

The permutation-based F-test was carried out as previously described. The observed F-statistic for each of the metrics is plotted below as a black dot, while the distribution of permuted test statistics is shown as a white violin plot. All metrics showed significant site effects. 

```{r}
ratio_stats = readRDS(here::here("results/avg_tensor_by_roi_wide_F_df.rds"))
null_dist = readRDS(here::here("results/avg_tensor_by_roi_wide_null_F_df.rds")) %>% 
  mutate(outcome = map_chr(outcome, unique)) %>% 
  select(-rowname)
```

### All sites (raw data)

#### F Distributions {.tabset}

##### Plot

```{r, warning=FALSE}
plot_F_dist <- function(ratio_stats, null_dist){
  null_dist %>% 
  select(outcome, F) %>% 
  unnest() %>% 
  ggplot(aes(x=outcome, y=F)) + 
  geom_violin(draw_quantiles = c(0.95)) + 
  coord_flip() + 
  geom_point(aes(x=outcome, y=F), 
             data=tibble::rownames_to_column(ratio_stats)) 
}

plot_F_dist(ratio_stats, null_dist)
```

##### Table

```{r}
ratio_stats %>% 
  arrange(desc(F))
```


#### p-values {.tabset}

```{r}
#' get p value of F by comparing to permutation distribution
test_ratio_stat = function(ratio.stats, null.dists){
  
  p.vals = c()
  for (col in colnames(ratio.stats)){
    ratio.stat = pull(ratio.stats, col)
    null.dist = pull(null.dists, col)
    
    percentile = ecdf(null.dist)
    p.val <- (10000* (1 - percentile(ratio.stat)) + 1)/(1+10000)
    p.vals = c(p.vals, p.val)
  }
  names(p.vals) = colnames(ratio.stats)
  return(p.vals)
}

#' make a data.frame with p values
make_perm_df <- function(ratio_stats, null_dist){
  observed_F <- ratio_stats  %>%
  select(outcome, F) %>% 
  pivot_wider(names_from = 'outcome', values_from = 'F')


  null_Fs <- null_dist %>% 
    select(outcome, F) %>% 
    unnest() %>% 
    pivot_wider(names_from = 'outcome', values_from = 'F') %>% 
    unnest() %>% 
    select(!!!colnames(observed_F)) # re-order columns to match observed
  
  p_df <- data.frame(pval = test_ratio_stat(observed_F, null_Fs))
  
  perm_df <- cbind(
    ratio_stats %>% 
      select(F),
    p_df %>% 
      mutate(pval_fdr = p.adjust(pval, method = 'fdr'))
  )
  return(perm_df)
}

perm_df <- make_perm_df(ratio_stats, null_dist)
```

##### Plot 

```{r}
plot_p_perm <- function(perm_df){
  perm_df %>% 
    tibble::rownames_to_column('outcome') %>% 
    mutate(outcome = fct_reorder(outcome, pval_fdr, .desc = TRUE)) %>% 
    ggplot(aes(x=outcome, y=pval_fdr)) + 
    geom_point(size = 2) +
    geom_hline(yintercept = 0.05, color = 'red') + 
    coord_flip() + 
    ggtitle("P-values of Permutation Test") +
    ylim(0,1)
}

plot_p_perm(perm_df)
```

##### Table

```{r}
perm_df %>% 
  arrange(desc(pval))
```


#### Effect size: f2

```{r}
### Make this into a function
# Full models

#' get f2 via Nakagawa's R2 for mixed models
#' 
#' @param data my dataframe 
#' @param r2_type the type of R2 to calculate: 'c' for conditional or 'm' for marginal (see Nakagawa, S., and Schielzeth, H. (2013) for details)
get_f2 <- function(data, r2_type = 'c'){
  run_full_models <- function(data){
    models <- outcomes %>% 
     lapply(function(x) lmer(reformulate("(1|subject) + site", x), data=data)) %>% 
     setNames(outcomes)
    return(models)
  }
  
  # Null models
  run_null_models <- function(data){
    models_0 <- outcomes %>% 
      lapply(function(x) lmer(reformulate("(1|subject)", x), data=data)) %>% 
      setNames(outcomes)
    return(models_0)
  }
  
  get_r2 <- function(mod){
    case_when(r2_type == 'c' ~ performance::r2_nakagawa(mod)$R2_conditional, 
              r2_type == 'm' ~ performance::r2_nakagawa(mod)$R2_marginal)
  } 
  
  models <- run_full_models(data)
  models_0 <- run_null_models(data)
  
  R2_ab <- map_dbl(models, get_r2)
  R2_a <- map_dbl(models_0, get_r2)
  
  f2 <- (R2_ab - R2_a)/(1 - R2_ab)
  
  data.frame(f2 = f2, 
             R2_ab = R2_ab,
             R2_a = R2_a)
}

get_f2(df) %>% 
  arrange(desc(f2))
```

#### FK test

```{r}
run_FK_test <- function(data){
  run_full_models <- function(data){
    models <- outcomes %>% 
     lapply(function(x) lmer(reformulate("(1|subject) + site", x), data=data)) %>% 
     setNames(outcomes)
    return(models)
  }

  models <- run_full_models(data)
  
  FK_df <- map(models, ~ cbind(.x@frame, data.frame(resid = residuals(.x)))) %>% # make residual column for each @frame in model output
    map(~fligner.test(.x$resid, .x$site)) %>% # run FK test on all residuals
    map(~data.frame(ChiSq = .x$statistic, DF = .x$parameter, pval = .x$p.value)) %>% # extract results from FK tests
    bind_rows(.id = 'outcome')
  
  # correct pvalues
  FK_df <- FK_df %>% 
    mutate(pval_fdr = p.adjust(pval, method='fdr'))
  rownames(FK_df) <- NULL
  return(FK_df)
}
  
df %>% 
  run_FK_test() %>% 
  arrange(pval)
```

### No Hopkins (raw)

Image acquisition at Hopkins was performed with a Philips scanner and distortion correction during on-scanner reconstruction. As the scanner manufacturer and reconstructions method differed than other sites, here we report the permutation results based on the non-Hopkins data only.

```{r}
ratio_stats = readRDS(here::here("results/avg_tensor_by_roi_wide_no_Hopkins_F_df.rds"))
null_dist = readRDS(here::here("results/avg_tensor_by_roi_wide_no_Hopkins_null_F_df.rds"))  %>% 
  mutate(outcome = map_chr(outcome, unique)) %>% 
  select(-rowname)
```

#### F Distributions {.tabset}

##### Plot

```{r, warning=FALSE}
plot_F_dist(ratio_stats, null_dist)
```

##### Table

```{r}
ratio_stats %>% 
  arrange(desc(F))
```


#### p-values {.tabset}

```{r}
perm_df <- make_perm_df(ratio_stats, null_dist)
```

##### Plot 

```{r}
plot_p_perm(perm_df)
```

##### Table

```{r}
perm_df %>% 
  arrange(desc(pval))
```

#### Effect size: f2

```{r}
df %>% 
  filter(site != 'Hopkins') %>% 
  get_f2() %>% 
  arrange(desc(f2))
```

#### FK test

```{r}
df %>% 
  filter(site != 'Hopkins') %>% 
  run_FK_test() %>% 
  arrange(pval)
```

### All site  (harmonized)

```{r}
ratio_stats = readRDS(here::here("results/avg_tensor_by_roi_wide_harmonized_F_df.rds"))
null_dist = readRDS(here::here("results/avg_tensor_by_roi_wide_harmonized_null_F_df.rds")) %>% 
  mutate(outcome = map_chr(outcome, unique)) %>% 
  select(-rowname)
```

#### F Distributions {.tabset}

##### Plot

```{r, warning=FALSE}
plot_F_dist(ratio_stats, null_dist)
```

##### Table

```{r}
ratio_stats %>% 
  arrange(desc(F))
```


#### p-values {.tabset}

```{r}
perm_df <- make_perm_df(ratio_stats, null_dist)
```

##### Plot 

```{r}
plot_p_perm(perm_df)
```

##### Table

```{r}
perm_df %>% 
  arrange(desc(pval))
```

#### Effect size: f2

```{r}
harmonized_df %>% 
  get_f2() %>% 
  arrange(desc(f2))
```

#### FK test

```{r}
harmonized_df %>% 
  run_FK_test() %>% 
  arrange(pval)
```

### No Hopkins  (harmonized)

```{r}
ratio_stats = readRDS(here::here("results/avg_tensor_by_roi_wide_no_Hopkins_harmonized_F_df.rds"))
null_dist = readRDS(here::here("results/avg_tensor_by_roi_wide_no_Hopkins_harmonized_null_F_df.rds")) %>% 
  mutate(outcome = map_chr(outcome, unique)) %>% 
  select(-rowname)
```

#### F Distributions {.tabset}

##### Plot

```{r, warning=FALSE}
plot_F_dist(ratio_stats, null_dist)
```

##### Table

```{r}
ratio_stats %>% 
  arrange(desc(F))
```


#### p-values {.tabset}

```{r}
perm_df <- make_perm_df(ratio_stats, null_dist)
```

##### Plot 

```{r}
plot_p_perm(perm_df)
```

##### Table

```{r}
perm_df %>% 
  arrange(desc(pval))
```

#### Effect size: f2

```{r}
harmonized_no_hopkins_df %>% 
  get_f2() %>% 
  arrange(desc(f2))
```

#### FK test

```{r}
harmonized_no_hopkins_df %>% 
  run_FK_test() %>% 
  arrange(pval)
```

## Mixed Models

For each of the 36 outcome variables, two mixed models (a base and an extended model) were fit and the extended model of interest was compared against the null/base model using a likelihood ratio test. The resulting p-values are reported. ICCs based on the extented model are also reported. 

### All Sites (raw data)

Of note, two of the models estimated a 0 variance for the site random effect. P-values and ICCs are not provided for these two models.

```{r, warning=TRUE}
# Full models
run_full_models <- function(data){
  models <- outcomes %>% 
   lapply(function(x) lmer(reformulate("(1|subject) + (1|site)", x), data=data)) %>% 
   setNames(outcomes)
}

# Null models
run_null_models <- function(data){
  models_0 <- outcomes %>% 
    lapply(function(x) lmer(reformulate("(1|subject)", x), data=data)) %>% 
    setNames(outcomes)
}

#' Test variance term
#' @param mod full model
#' @param mod_0 null model
test_variance_term <- function(mod, mod_0){
  
  A = lme4:::anova.merMod(mod, mod_0)
  lrt_stat = as.numeric(-2*(logLik(mod_0) - logLik(mod)))
  
  #if(near(lrt_stat, A$Chisq[2])) return(NA)
  
  # Naive p-value given the chi_sq(df = 1) null distribution
  pval_naive = pchisq(q = lrt_stat, df = 1, lower.tail = FALSE) 
  
  # p-value given a 50:50 mixture of chisq(1) and chisq(2),
  # according to Self & Liang 1987
  pval_SL = pchisq(q = lrt_stat, df = 1, lower.tail = FALSE)/2 +
    pchisq(q = lrt_stat, df = 2, lower.tail = FALSE)/2
  
  return(data.frame(pval_naive = pval_naive,
                    pval_SL = pval_SL))
}

# Which random effect variance is 0 
get_zero_var_model_names <- function(models){
  get_var <- function(model) summary(model)$varcor$site[1,1]
  
  zero_var_models <- models %>% 
    purrr::map_dbl(get_var) %>% 
    as.data.frame() %>% 
    setNames('variance') %>% 
    rownames_to_column(var = 'outcome') %>% 
    filter(near(variance, 0, .Machine$double.eps)) %>% 
    pull(outcome)
  return(zero_var_models)
}

models <- run_full_models(df)
models_0 <- run_null_models(df)
zero_var_model_names <- get_zero_var_model_names(models)

singular_models <- names(which(map_lgl(models, isSingular)))
```

<details>
  <summary>See models with 0 variance</summary>
  
```{r}
for (name in zero_var_model_names){
  cat('###', name, '\n\n')
  print(summary(models[[name]]))
  cat("\n\n")
}
```
  
</details>

<details>
  <summary>See models with 'singular' warning</summary>
  
```{r}
for (name in singular_models){
  cat('###', name, '\n\n')
  print(summary(models[[name]]))
  cat("\n\n")
}
```
  
</details>

<details>
  <summary>See model residuals</summary>
  
```{r,}
plot_resid <- function(mod, name) car::qqPlot(residuals(mod),dist="norm", main=glue::glue("{name} Residuals"))
imap(models, plot_resid)
```
  
</details>


#### p-values {.tabset}

```{r}
make_SL_df <- function(models, models_0){
  purrr::map2(models, models_0, test_variance_term) %>% 
    setNames(outcomes) %>% 
    bind_rows(.id = 'outcome') %>% 
    #filter(!(outcome %in% zero_var_model_names)) %>% 
    select(-pval_naive) %>% 
    rename(pval = pval_SL) %>% 
    mutate(pval_fdr = p.adjust(pval, method = 'fdr')) %>% 
    arrange(pval)
}

p_df <- make_SL_df(models, models_0)

```

##### Plot

```{r}
plot_p_SL <- function(p_df){
  p_df %>% 
    mutate(outcome = fct_reorder(outcome, pval_fdr, .desc = TRUE)) %>% 
    ggplot(aes(x=outcome, y=pval_fdr)) + 
    geom_point(size = 2) +
    geom_hline(yintercept = 0.05, color = 'red') + 
    coord_flip() + 
    ggtitle("P-values of Likelihood Ratio Test (Self & Liang 1987)") +
    ylim(0,1)
}

plot_p_SL(p_df)
```

##### Table

```{r}
options(scipen = 999)
p_df
```

#### Effect size: ICC {.tabset}

ICC for the 35 remaining models is shown

```{r}
options(scipen=999)
#' Get ICC for a single model
get_icc <- function(df){

  outcomes %>% 
    map(~lmer(reformulate("(1|subject) + (1|site)", .x), data=df)) %>% 
    purrr::map(~ performance::icc(.x, by_group = TRUE) %>% as.data.frame) %>% 
    setNames(outcomes) %>% 
    bind_rows(.id = "outcome") %>% 
    select_if(~ !all(is.na(.))) %>% # eliminate column of NAs
    na.omit() %>% # eliminate rows with NA
    pivot_wider(names_from = 'Group', values_from = 'ICC', names_prefix = 'ICC_') %>% 
    relocate(ICC_site, .after = 'outcome') %>%
    arrange(desc(ICC_site))
}
# extract observed icc_df
(icc_df <- get_icc(df))
```


##### Site

```{r}
## Make violin plots
## AND make line
plot_ICC_site <- function(icc_df, boot_df){
  icc_df %>% 
    left_join(boot_df %>% 
                select(-ICC_site, -ICC_subject), by = 'outcome') %>% 
    mutate(outcome = fct_reorder(outcome, ICC_site)) %>% 
    ggplot(aes(x=outcome, y=ICC_site)) + 
    geom_errorbar(aes(ymin=lwr_site, ymax=uppr_site)) +
    geom_point() + 
    coord_flip() + 
    ggtitle("ICC of Site Random Effect") + 
    ylim(0,1)
}

boot_df <- readRDS(here::here('results/avg_tensor_by_roi_wide_boot_icc.rds'))
plot_ICC_site(icc_df, boot_df)
```

##### Subject

```{r}
plot_ICC_subject <- function(icc_df, boot_df){
  icc_df %>% 
    left_join(boot_df %>% 
              select(-ICC_site, -ICC_subject), by = 'outcome') %>% 
    mutate(outcome = fct_reorder(outcome, ICC_subject)) %>% 
    ggplot(aes(x=outcome, y=ICC_subject)) + 
    geom_errorbar(aes(ymin=lwr_subject, ymax=uppr_subject)) +
    geom_point() + 
    coord_flip() + 
    ggtitle("ICC of Subject Random Effect") + 
    ylim(0,1)
}
plot_ICC_subject(icc_df, boot_df)
```

After FDR-adjustment, 34 of the 36 variables show site effects according to the mixed models. 

### No Hopkins (raw data)

Here, we report results from the mixed models carried out only on the non-Hopkins data.

```{r, warning=TRUE}
models <- df %>% 
  filter(site != 'Hopkins') %>% 
  run_full_models()

models_0 <- df %>% 
  filter(site != 'Hopkins') %>% 
  run_null_models()

zero_var_model_names <- get_zero_var_model_names(models)
singular_models <- names(which(map_lgl(models, isSingular)))
```

<details>
  <summary>See models with 0 variance</summary>
  
```{r}
for (name in zero_var_model_names){
  cat('###', name, '\n\n')
  print(summary(models[[name]]))
  cat("\n\n")
}
```
  
</details>

<details>
  <summary>See models with 'singular' warning</summary>
  
```{r}
for (name in singular_models){
  cat('###', name, '\n\n')
  print(summary(models[[name]]))
  cat("\n\n")
}
```
  
</details>

<details>
  <summary>See model residuals</summary>
  
```{r,}
plot_resid <- function(mod, name) car::qqPlot(residuals(mod),dist="norm", main=glue::glue("{name} Residuals"))
imap(models, plot_resid)
```
  
</details>

#### p-values {.tabset}

```{r}
p_df <- make_SL_df(models, models_0)
```

##### Plot

```{r}
plot_p_SL(p_df)
```

##### Table

```{r}
options(scipen = 999)
p_df %>% 
  arrange(pval_fdr)
```


#### Effect size: ICC {.tabset}

ICC for the 35 remaining models is shown below


```{r}
# extract icc_df
icc_df <- df %>% 
  filter(site != 'Hopkins') %>% 
  get_icc()

icc_df
```

##### Site

```{r}
options(scipen=999)
boot_df <- readRDS(here::here('results/avg_tensor_by_roi_wide_no_Hopkins_boot_icc.rds'))
plot_ICC_site(icc_df, boot_df)
```

##### Subject

```{r}
plot_ICC_subject(icc_df, boot_df)
```

After FDR-adjustment, 14 of the 36 variables show site effects according to the mixed models run on non-Hopkins data.

### All sites (harmonized data)

Here, we report results from the mixed models carried out only on the non-Hopkins data.

```{r, warning=TRUE}
models <- harmonized_df %>% 
  run_full_models()

models_0 <- harmonized_df %>% 
  run_null_models()

zero_var_model_names <- get_zero_var_model_names(models)
singular_models <- names(which(map_lgl(models, isSingular)))
```

<details>
  <summary>See models with 0 variance</summary>
  
```{r}
for (name in zero_var_model_names){
  cat('###', name, '\n\n')
  print(summary(models[[name]]))
  cat("\n\n")
}
```
  
</details>

<details>
  <summary>See models with 'singular' warning</summary>
  
```{r}
for (name in singular_models){
  cat('###', name, '\n\n')
  print(summary(models[[name]]))
  cat("\n\n")
}
```
  
</details>

<details>
  <summary>See model residuals</summary>
  
```{r,}
plot_resid <- function(mod, name) car::qqPlot(residuals(mod),dist="norm", main=glue::glue("{name} Residuals"))
imap(models, plot_resid)
```
  
</details>

#### p-values {.tabset}

```{r}
p_df <- make_SL_df(models, models_0)
```

##### Plot

```{r}
plot_p_SL(p_df)
```

##### Table

```{r}
options(scipen = 999)
p_df %>% 
  arrange(pval_fdr)
```


#### Effect size: ICC {.tabset}

ICC for the 35 remaining models is shown below

```{r}
# extract icc_df
icc_df <- harmonized_df %>% 
  get_icc()

icc_df
```

##### Site

```{r}
options(scipen=999)
boot_df <- readRDS(here::here('results/avg_tensor_by_roi_wide_harmonized_boot_icc.rds'))
plot_ICC_site(icc_df, boot_df)
```

##### Subject

```{r}
plot_ICC_subject(icc_df, boot_df)
```

After FDR-adjustment, 14 of the 36 variables show site effects according to the mixed models run on non-Hopkins data.

### No Hopkins (harmonized)

Here, we report results from the mixed models carried out only on the non-Hopkins data.

```{r}
models <- harmonized_no_hopkins_df %>% 
  run_full_models()

models_0 <- harmonized_no_hopkins_df %>% 
  run_null_models()

zero_var_model_names <- get_zero_var_model_names(models)
singular_models <- names(which(map_lgl(models, isSingular)))
```

<details>
  <summary>See models with 0 variance</summary>
  
```{r}
for (name in zero_var_model_names){
  cat('###', name, '\n\n')
  print(summary(models[[name]]))
  cat("\n\n")
}
```
  
</details>

<details>
  <summary>See models with 'singular' warning</summary>
  
```{r}
for (name in singular_models){
  cat('###', name, '\n\n')
  print(summary(models[[name]]))
  cat("\n\n")
}
```
  
</details>

<details>
  <summary>See model residuals</summary>
  
```{r,}
plot_resid <- function(mod, name) car::qqPlot(residuals(mod),dist="norm", main=glue::glue("{name} Residuals"))
imap(models, plot_resid)
```
  
</details>

#### p-values {.tabset}

```{r}
p_df <- make_SL_df(models, models_0)
```

##### Plot

```{r}
plot_p_SL(p_df)
```

##### Table

```{r}
options(scipen = 999)
p_df %>% 
  arrange(pval_fdr)
```


#### Effect size: ICC {.tabset}

ICC for the 35 remaining models is shown below

```{r}
# extract icc_df
icc_df <- harmonized_no_hopkins_df %>% 
  get_icc()

icc_df
```

##### Site

```{r}
options(scipen=999)
boot_df <- readRDS(here::here('results/avg_tensor_by_roi_wide_no_Hopkins_harmonized_boot_icc.rds'))
plot_ICC_site(icc_df, boot_df)
```

##### Subject

```{r}
plot_ICC_subject(icc_df, boot_df)
```

After FDR-adjustment, 14 of the 36 variables show site effects according to the mixed models run on non-Hopkins data.
